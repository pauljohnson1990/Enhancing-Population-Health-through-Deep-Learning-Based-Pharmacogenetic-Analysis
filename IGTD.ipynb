{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85fdf25d",
      "metadata": {
        "id": "85fdf25d"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import spearmanr, rankdata\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import time\n",
        "import _pickle as cp\n",
        "import sys\n",
        "from astropy.stats import median_absolute_deviation\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9b705c4",
      "metadata": {
        "id": "a9b705c4"
      },
      "outputs": [],
      "source": [
        "def LoadData(fileName):\n",
        "    # LoadData loads the relevant files.\n",
        "    # Input: fileName - the name of the file to load\n",
        "    # Output: exprData - the imputed expression data matrix\n",
        "    #         residuals - the residuals (dependent variable)\n",
        "    #         geneTissue - the names of the gene-tissue names (feature names)\n",
        "\n",
        "    with open(fileName, 'r') as fid:\n",
        "        tissues = fid.readline().strip().split('\\t')\n",
        "        geneNames = fid.readline().strip().split('\\t')\n",
        "        geneTissue = [tissues[i] + geneNames[i] for i in range(1, len(tissues))]\n",
        "\n",
        "    data = np.loadtxt(fileName, delimiter='\\t', skiprows=2)\n",
        "    residuals = data[:, 0]\n",
        "    exprData = data[:, 1:]\n",
        "\n",
        "    return exprData, residuals, geneTissue\n",
        "\n",
        "aa_expr,aa_res,aa_gt=LoadData('AA_train.txt')\n",
        "aa_expr_test,aa_res_test,_=LoadData('AA_validation.txt')\n",
        "e_expr,e_res,e_gt=LoadData('EUR_train.txt')\n",
        "e_expr_test,e_res_test,_=LoadData('EUR_validation.txt')\n",
        "# X = np.vstack((aa_expr,e_expr,aa_expr_test,e_expr_test))\n",
        "# y = np.concatenate((aa_res,e_res,aa_res_test,e_res_test))\n",
        "\n",
        "\n",
        "\n",
        "XAA = np.vstack((aa_expr,aa_expr_test))\n",
        "yAA = np.concatenate((aa_res,aa_res_test))\n",
        "XEUR = np.vstack((e_expr,e_expr_test))\n",
        "yEUR = np.concatenate((e_res,e_res_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9624ed79",
      "metadata": {
        "id": "9624ed79"
      },
      "outputs": [],
      "source": [
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# #scaler = StandardScaler()\n",
        "# scaler.fit(X)\n",
        "# X_scaled = scaler.transform(X)\n",
        "# data = X_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c97f4e9",
      "metadata": {
        "id": "6c97f4e9"
      },
      "outputs": [],
      "source": [
        "def select_features_by_variation(data, variation_measure='var', threshold=None, num=None, draw_histogram=False,\n",
        "                                 bins=100, log=False):\n",
        "    '''\n",
        "    This function evaluates the variations of individual features and returns the indices of features with large\n",
        "    variations. Missing values are ignored in evaluating variation.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data: numpy array or pandas data frame of numeric values, with a shape of [n_samples, n_features].\n",
        "    variation_metric: string indicating the metric used for evaluating feature variation. 'var' indicates variance;\n",
        "        'std' indicates standard deviation; 'mad' indicates median absolute deviation. Default is 'var'.\n",
        "    threshold: float. Features with a variation larger than threshold will be selected. Default is None.\n",
        "    num: positive integer. It is the number of features to be selected based on variation.\n",
        "        The number of selected features will be the smaller of num and the total number of\n",
        "        features with non-missing variations. Default is None. threshold and portion can not take values\n",
        "        and be used simultaneously.\n",
        "    draw_histogram: boolean, whether to draw a histogram of feature variations. Default is False.\n",
        "    bins: positive integer, the number of bins in the histogram. Default is the smaller of 50 and the number of\n",
        "        features with non-missing variations.\n",
        "    log: boolean, indicating whether the histogram should be drawn on log scale.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    indices: 1-D numpy array containing the indices of selected features. If both threshold and\n",
        "        portion are None, indices will be an empty array.\n",
        "    '''\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data = data.values\n",
        "    elif not isinstance(data, np.ndarray):\n",
        "        print('Input data must be a numpy array or pandas data frame')\n",
        "        sys.exit(1)\n",
        "\n",
        "    if variation_measure == 'std':\n",
        "        v_all = np.nanstd(a=data, axis=0)\n",
        "    elif variation_measure == 'mad':\n",
        "        v_all = median_absolute_deviation(data=data, axis=0, ignore_nan=True)\n",
        "    else:\n",
        "        v_all = np.nanvar(a=data, axis=0)\n",
        "\n",
        "    indices = np.where(np.invert(np.isnan(v_all)))[0]\n",
        "    v = v_all[indices]\n",
        "\n",
        "    if draw_histogram:\n",
        "        if len(v) < 50:\n",
        "            print('There must be at least 50 features with variation measures to draw a histogram')\n",
        "        else:\n",
        "            bins = int(min(bins, len(v)))\n",
        "            _ = plt.hist(v, bins=bins, log=log)\n",
        "            plt.show()\n",
        "\n",
        "    if threshold is None and num is None:\n",
        "        return np.array([])\n",
        "    elif threshold is not None and num is not None:\n",
        "        print('threshold and portion can not be used simultaneously. Only one of them can take a real value')\n",
        "        sys.exit(1)\n",
        "\n",
        "    if threshold is not None:\n",
        "        indices = indices[np.where(v > threshold)[0]]\n",
        "    else:\n",
        "        n_f = int(min(num, len(v)))\n",
        "        indices = indices[np.argsort(-v)[:n_f]]\n",
        "\n",
        "    indices = np.sort(indices)\n",
        "\n",
        "    return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd26e1d8",
      "metadata": {
        "id": "cd26e1d8"
      },
      "outputs": [],
      "source": [
        "def generate_feature_distance_ranking(data, method='Pearson'):\n",
        "    '''\n",
        "    This function generates ranking of distances/dissimilarities between features for tabular data.\n",
        "\n",
        "    Input:\n",
        "    data: input data, n_sample by n_feature\n",
        "    method: 'Euclidean' calculates similarity between features based on Euclidean distance;\n",
        "        'Pearson' uses Pearson correlation coefficient to evaluate similarity between features;\n",
        "        'Spearman' uses Spearman correlation coefficient to evaluate similarity between features;\n",
        "        'set' uses Jaccard index to evaluate similarity between features that are binary variables.\n",
        "\n",
        "    Return:\n",
        "    ranking: symmetric ranking matrix based on dissimilarity\n",
        "    corr: matrix of distances between features\n",
        "    '''\n",
        "\n",
        "    num = data.shape[1]\n",
        "    if method == 'Pearson':\n",
        "        corr = np.corrcoef(np.transpose(data))\n",
        "    elif method == 'Spearman':\n",
        "        corr = spearmanr(data).correlation\n",
        "    elif method == 'Euclidean':\n",
        "        corr = squareform(pdist(np.transpose(data), metric='euclidean'))\n",
        "        corr = np.max(corr) - corr\n",
        "        corr = corr / np.max(corr)\n",
        "    elif method == 'set':  # This is the new set operation to calculate similarity. It does not tolerate all-zero features.\n",
        "        corr1 = np.dot(np.transpose(data), data)\n",
        "        corr2 = data.shape[0] - np.dot(np.transpose(1 - data), 1 - data)\n",
        "        corr = corr1 / corr2\n",
        "\n",
        "    corr = 1 - corr\n",
        "    corr = np.around(a=corr, decimals=10)\n",
        "\n",
        "    tril_id = np.tril_indices(num, k=-1)\n",
        "    rank = rankdata(corr[tril_id])\n",
        "    ranking = np.zeros((num, num))\n",
        "    ranking[tril_id] = rank\n",
        "    ranking = ranking + np.transpose(ranking)\n",
        "\n",
        "    return ranking, corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecf1ea03",
      "metadata": {
        "id": "ecf1ea03"
      },
      "outputs": [],
      "source": [
        "def generate_matrix_distance_ranking(num_r, num_c, method='Euclidean', num=None):\n",
        "    '''\n",
        "    This function calculates the ranking of distances between all pairs of entries in a matrix of size num_r by num_c.\n",
        "\n",
        "    Input:\n",
        "    num_r: number of rows in the matrix\n",
        "    num_c: number of columns in the matrix\n",
        "    method: method used to calculate distance. Can be 'Euclidean' or 'Manhattan'.\n",
        "    num: number of real features. If None, num = num_r * num_c. If num < num_r * num_c, num_r * num_c - num\n",
        "        zeros will be padded to the image representation.\n",
        "\n",
        "    Return:\n",
        "    coordinate: a num-by-2 matrix giving the coordinates of elements in the matrix.\n",
        "    ranking: a num-by-num matrix giving the ranking of pair-wise distance.\n",
        "\n",
        "    '''\n",
        "\n",
        "    if num is None:\n",
        "        num = num_r * num_c\n",
        "\n",
        "    # generate the coordinates of elements in a matrix\n",
        "    for r in range(num_r):\n",
        "        if r == 0:\n",
        "            coordinate = np.transpose(np.vstack((np.zeros(num_c), range(num_c))))\n",
        "        else:\n",
        "            coordinate = np.vstack((coordinate, np.transpose(np.vstack((np.ones(num_c) * r, range(num_c))))))\n",
        "    coordinate = coordinate[:num, :]\n",
        "\n",
        "    # calculate the closeness of the elements\n",
        "    cord_dist = np.zeros((num, num))\n",
        "    if method == 'Euclidean':\n",
        "        for i in range(num):\n",
        "            cord_dist[i, :] = np.sqrt(np.square(coordinate[i, 0] * np.ones(num) - coordinate[:, 0]) +\n",
        "                                     np.square(coordinate[i, 1] * np.ones(num) - coordinate[:, 1]))\n",
        "    elif method == 'Manhattan':\n",
        "        for i in range(num):\n",
        "            cord_dist[i, :] = np.abs(coordinate[i, 0] * np.ones(num) - coordinate[:, 0]) + \\\n",
        "                             np.abs(coordinate[i, 1] * np.ones(num) - coordinate[:, 1])\n",
        "\n",
        "    # generate the ranking based on distance\n",
        "    tril_id = np.tril_indices(num, k=-1)\n",
        "    rank = rankdata(cord_dist[tril_id])\n",
        "    ranking = np.zeros((num, num))\n",
        "    ranking[tril_id] = rank\n",
        "    ranking = ranking + np.transpose(ranking)\n",
        "\n",
        "    coordinate = np.int64(coordinate)\n",
        "    return (coordinate[:, 0], coordinate[:, 1]), ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82e99532",
      "metadata": {
        "id": "82e99532"
      },
      "outputs": [],
      "source": [
        "def IGTD_square_error(source, target, max_step=1000, switch_t=0, val_step=50, min_gain=0.00001, random_state=1,\n",
        "                      save_folder=None, file_name=''):\n",
        "    '''\n",
        "    This function switches the order of rows (columns) in the source ranking matrix to make it similar to the target\n",
        "    ranking matrix. In each step, the algorithm randomly picks a row that has not been switched with others for\n",
        "    the longest time and checks all possible switch of this row, and selects the switch that reduces the\n",
        "    dissimilarity most. Dissimilarity (i.e. the error) is the summation of squared difference of\n",
        "    lower triangular elements between the rearranged source ranking matrix and the target ranking matrix.\n",
        "\n",
        "    Input:\n",
        "    source: a symmetric ranking matrix with zero diagonal elements.\n",
        "    target: a symmetric ranking matrix with zero diagonal elements. 'source' and 'target' should have the same size.\n",
        "    max_step: the maximum steps that the algorithm should run if never converges.\n",
        "    switch_t: the threshold to determine whether feature switching should happen\n",
        "    val_step: number of steps for checking gain on the objective function to determine convergence\n",
        "    min_gain: if the objective function is not improved more than 'min_gain' in 'val_step' steps,\n",
        "        the algorithm terminates.\n",
        "    random_state: for setting random seed.\n",
        "    save_folder: a path to save the picture of source ranking matrix in the optimization process.\n",
        "    file_name: a string as part of the file names for saving results\n",
        "\n",
        "    Return:\n",
        "    index_record: ordering index to rearrange the rows(columns) in 'source' in the optimization process\n",
        "    err_record: the error history in the optimization process\n",
        "    run_time: the time at which each step is finished in the optimization process\n",
        "    '''\n",
        "\n",
        "\n",
        "    np.random.RandomState(seed=random_state)\n",
        "    if os.path.exists(save_folder):\n",
        "        shutil.rmtree(save_folder)\n",
        "    os.mkdir(save_folder)\n",
        "\n",
        "    source = source.copy()\n",
        "    num = source.shape[0]\n",
        "    tril_id = np.tril_indices(num, k=-1)\n",
        "    index = np.array(range(num))\n",
        "    index_record = np.empty((max_step + 1, num))\n",
        "    index_record.fill(np.nan)\n",
        "    index_record[0, :] = index.copy()\n",
        "\n",
        "    # calculate the error associated with each row\n",
        "    err_v = np.empty(num)\n",
        "    err_v.fill(np.nan)\n",
        "    for i in range(num):\n",
        "        err_v[i] = np.sum(np.square(source[i, 0:i] - target[i, 0:i])) + \\\n",
        "                   np.sum(np.square(source[(i + 1):, i] - target[(i + 1):, i]))\n",
        "\n",
        "    step_record = -np.ones(num)\n",
        "    err_record = [np.sum(np.square(source[tril_id] - target[tril_id]))]\n",
        "    pre_err = err_record[0]\n",
        "    t1 = time.time()\n",
        "    run_time = [0]\n",
        "\n",
        "    for s in range(max_step):\n",
        "        delta = - np.ones(num) * np.inf\n",
        "\n",
        "        # randomly pick a row that has not been considered for the longest time\n",
        "        idr = np.where(step_record == np.min(step_record))[0]\n",
        "        ii = idr[np.random.permutation(len(idr))[0]]\n",
        "\n",
        "        for jj in range(num):\n",
        "            if jj == ii:\n",
        "                continue\n",
        "\n",
        "            if ii < jj:\n",
        "                i = ii\n",
        "                j = jj\n",
        "            else:\n",
        "                i = jj\n",
        "                j = ii\n",
        "\n",
        "            err_ori = err_v[i] + err_v[j] - np.square(source[j, i] - target[j, i])\n",
        "\n",
        "            err_i = np.sum(np.square(source[j, :i] - target[i, :i])) + \\\n",
        "                    np.sum(np.square(source[(i + 1):j, j] - target[(i + 1):j, i])) + \\\n",
        "                    np.sum(np.square(source[(j + 1):, j] - target[(j + 1):, i])) + np.square(source[i, j] - target[j, i])\n",
        "            err_j = np.sum(np.square(source[i, :i] - target[j, :i])) + \\\n",
        "                    np.sum(np.square(source[i, (i + 1):j] - target[j, (i + 1):j])) + \\\n",
        "                    np.sum(np.square(source[(j + 1):, i] - target[(j + 1):, j])) + np.square(source[i, j] - target[j, i])\n",
        "            err_test = err_i + err_j - np.square(source[i, j] - target[j, i])\n",
        "\n",
        "            delta[jj] = err_ori - err_test\n",
        "\n",
        "        delta_norm = delta / pre_err\n",
        "        id = np.where(delta_norm >= switch_t)[0]\n",
        "        if len(id) > 0:\n",
        "            jj = np.argmax(delta)\n",
        "\n",
        "            # Update the error associated with each row\n",
        "            if ii < jj:\n",
        "                i = ii\n",
        "                j = jj\n",
        "            else:\n",
        "                i = jj\n",
        "                j = ii\n",
        "            for k in range(num):\n",
        "                if k < i:\n",
        "                    err_v[k] = err_v[k] - np.square(source[i, k] - target[i, k]) - np.square(source[j, k] - target[j, k]) + \\\n",
        "                               np.square(source[j, k] - target[i, k]) + np.square(source[i, k] - target[j, k])\n",
        "                elif k == i:\n",
        "                    err_v[k] = np.sum(np.square(source[j, :i] - target[i, :i])) + \\\n",
        "                        np.sum(np.square(source[(i + 1):j, j] - target[(i + 1):j, i])) + \\\n",
        "                        np.sum(np.square(source[(j + 1):, j] - target[(j + 1):, i])) + np.square(source[i, j] - target[j, i])\n",
        "                elif k < j:\n",
        "                    err_v[k] = err_v[k] - np.square(source[k, i] - target[k, i]) - np.square(source[j, k] - target[j, k]) + \\\n",
        "                               np.square(source[k, j] - target[k, i]) + np.square(source[i, k] - target[j, k])\n",
        "                elif k == j:\n",
        "                    err_v[k] = np.sum(np.square(source[i, :i] - target[j, :i])) + \\\n",
        "                        np.sum(np.square(source[i, (i + 1):j] - target[j, (i + 1):j])) + \\\n",
        "                        np.sum(np.square(source[(j + 1):, i] - target[(j + 1):, j])) + np.square(source[i, j] - target[j, i])\n",
        "                else:\n",
        "                    err_v[k] = err_v[k] - np.square(source[k, i] - target[k, i]) - np.square(source[k, j] - target[k, j]) + \\\n",
        "                               np.square(source[k, j] - target[k, i]) + np.square(source[k, i] - target[k, j])\n",
        "\n",
        "            # switch rows i and j\n",
        "            ii_v = source[ii, :].copy()\n",
        "            jj_v = source[jj, :].copy()\n",
        "            source[ii, :] = jj_v\n",
        "            source[jj, :] = ii_v\n",
        "            ii_v = source[:, ii].copy()\n",
        "            jj_v = source[:, jj].copy()\n",
        "            source[:, ii] = jj_v\n",
        "            source[:, jj] = ii_v\n",
        "            err = pre_err - delta[jj]\n",
        "\n",
        "            # update rearrange index\n",
        "            t = index[ii]\n",
        "            index[ii] = index[jj]\n",
        "            index[jj] = t\n",
        "\n",
        "            # update step record\n",
        "            step_record[ii] = s\n",
        "            step_record[jj] = s\n",
        "        else:\n",
        "            # error is not changed due to no switch\n",
        "            err = pre_err\n",
        "\n",
        "            # update step record\n",
        "            step_record[ii] = s\n",
        "\n",
        "        err_record.append(err)\n",
        "        print('Step ' + str(s) + ' err: ' + str(err))\n",
        "        index_record[s + 1, :] = index.copy()\n",
        "        run_time.append(time.time() - t1)\n",
        "\n",
        "        if s > val_step:\n",
        "            if np.sum((err_record[-val_step - 1] - np.array(err_record[(-val_step):])) / err_record[\n",
        "                -val_step - 1] >= min_gain) == 0:\n",
        "                break\n",
        "\n",
        "        pre_err = err\n",
        "\n",
        "    index_record = index_record[:len(err_record), :].astype(np.int)\n",
        "    if save_folder is not None:\n",
        "        pd.DataFrame(index_record).to_csv(save_folder + '/' + file_name + '_index.txt', header=False, index=False,\n",
        "            sep='\\t', line_terminator='\\r\\n')\n",
        "        pd.DataFrame(np.transpose(np.vstack((err_record, np.array(range(s + 2))))),\n",
        "            columns=['error', 'steps']).to_csv(save_folder + '/' + file_name + '_error_and_step.txt',\n",
        "            header=True, index=False, sep='\\t', line_terminator='\\r\\n')\n",
        "        pd.DataFrame(np.transpose(np.vstack((err_record, run_time))), columns=['error', 'run_time']).to_csv(\n",
        "            save_folder + '/' + file_name + '_error_and_time.txt', header=True, index=False, sep='\\t',\n",
        "            line_terminator='\\r\\n')\n",
        "\n",
        "    return index_record, err_record, run_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "052c0aab",
      "metadata": {
        "id": "052c0aab"
      },
      "outputs": [],
      "source": [
        "def IGTD(source, target, err_measure='abs', max_step=1000, switch_t=0, val_step=50, min_gain=0.00001, random_state=1,\n",
        "         save_folder=None, file_name=''):\n",
        "    '''\n",
        "    This is just a wrapper function that wraps the two search functions using different error measures.\n",
        "    '''\n",
        "\n",
        "    if err_measure == 'abs':\n",
        "        index_record, err_record, run_time = IGTD_absolute_error(source=source,\n",
        "            target=target, max_step=max_step, switch_t=switch_t, val_step=val_step, min_gain=min_gain,\n",
        "            random_state=random_state, save_folder=save_folder, file_name=file_name)\n",
        "    if err_measure == 'squared':\n",
        "        index_record, err_record, run_time = IGTD_square_error(source=source,\n",
        "            target=target, max_step=max_step, switch_t=switch_t, val_step=val_step, min_gain=min_gain,\n",
        "            random_state=random_state, save_folder=save_folder, file_name=file_name)\n",
        "\n",
        "    return index_record, err_record, run_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "729dc377",
      "metadata": {
        "id": "729dc377"
      },
      "outputs": [],
      "source": [
        "def generate_image_data(data, index, num_row, num_column, coord, image_folder=None, file_name=''):\n",
        "    '''\n",
        "    This function generates the data in image format according to rearrangement indices. It saves the data\n",
        "    sample-by-sample in both txt files and image files\n",
        "\n",
        "    Input:\n",
        "    data: original tabular data, 2D array or data frame, n_samples by n_features\n",
        "    index: indices of features obtained through optimization, according to which the features can be\n",
        "        arranged into a num_r by num_c image.\n",
        "    num_row: number of rows in image\n",
        "    num_column: number of columns in image\n",
        "    coord: coordinates of features in the image/matrix\n",
        "    image_folder: directory to save the image and txt data files. If none, no data file is saved\n",
        "    file_name: a string as a part of the file names to save data\n",
        "\n",
        "    Return:\n",
        "    image_data: the generated data, a 3D numpy array. The third dimension is across samples. The range of values\n",
        "        is [0, 255]. Small values actually indicate high values in the original data.\n",
        "    samples: the names of indices of the samples\n",
        "    '''\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        samples = data.index.map(np.str)\n",
        "        data = data.values\n",
        "    else:\n",
        "        samples = [str(i) for i in range(data.shape[0])]\n",
        "\n",
        "    if os.path.exists(image_folder):\n",
        "        shutil.rmtree(image_folder)\n",
        "    os.mkdir(image_folder)\n",
        "\n",
        "    data_2 = data.copy()\n",
        "    data_2 = data_2[:, index]\n",
        "    max_v = np.max(data_2)\n",
        "    min_v = np.min(data_2)\n",
        "    data_2 = 255 - (data_2 - min_v) / (max_v - min_v) * 255 # Black color in heatmap indicates high value\n",
        "\n",
        "    image_data = np.empty((num_row, num_column, data_2.shape[0]))\n",
        "    image_data.fill(np.nan)\n",
        "    for i in range(data_2.shape[0]):\n",
        "        data_i = np.empty((num_row, num_column))\n",
        "        data_i.fill(np.nan)\n",
        "        data_i[coord] = data_2[i, :]\n",
        "\n",
        "        # find nan in data_i and change them to 255\n",
        "        idd = np.where(np.isnan(data_i))\n",
        "        data_i[idd] = 255\n",
        "\n",
        "        image_data[:, :, i] = data_i\n",
        "        image_data[:, :, i] = 255 - image_data[:, :, i] # High values in the array format of image data correspond\n",
        "                                                        # to high values in tabular data\n",
        "        if image_folder is not None:\n",
        "            fig = plt.figure()\n",
        "            plt.imshow(data_i, cmap='gray', vmin=0, vmax=255)\n",
        "            plt.axis('scaled')\n",
        "            plt.savefig(fname=image_folder + '/' + file_name + '_' + samples[i] + '_image.png', bbox_inches='tight',\n",
        "                        pad_inches=0)\n",
        "            plt.close(fig)\n",
        "\n",
        "            pd.DataFrame(image_data[:, :, i], index=None, columns=None).to_csv(image_folder + '/' + file_name + '_'\n",
        "                + samples[i] + '_data.txt', header=None, index=None, sep='\\t', line_terminator='\\r\\n')\n",
        "\n",
        "    return image_data, samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87f406e7",
      "metadata": {
        "id": "87f406e7"
      },
      "outputs": [],
      "source": [
        "def table_to_image(norm_d, scale, fea_dist_method, image_dist_method, save_image_size, max_step, val_step, normDir,\n",
        "                   error, switch_t=0, min_gain=0.00001):\n",
        "    '''\n",
        "    This function converts tabular data into images using the IGTD algorithm.\n",
        "\n",
        "    Input:\n",
        "    norm_d: a 2D array or data frame, which is the tabular data. Its size is n_samples by n_features\n",
        "    scale: a list of two positive integers. It includes the numbers of pixel rows and columns in the image\n",
        "        representation. The total number of pixels should not be smaller than the number of features,\n",
        "        i.e. scale[0] * scale[1] >= n_features.\n",
        "    fea_dist_method: a string indicating the method used for calculating the pairwise distances between features,\n",
        "        for which there are three options.\n",
        "        'Pearson' uses the Pearson correlation coefficient to evaluate the similarity between features.\n",
        "        'Spearman' uses the Spearman correlation coefficient to evaluate the similarity between features.\n",
        "        'set' uses the Jaccard index to evaluate the similarity between features that are binary variables.\n",
        "    image_dist_method: a string indicating the method used for calculating the distances between pixels in image.\n",
        "        It can be either 'Euclidean' or 'Manhattan'.\n",
        "    save_image_size: size of images (in inches) for saving visual results.\n",
        "    max_step: the maximum number of iterations that the IGTD algorithm will run if never converges.\n",
        "    val_step: the number of iterations for determining algorithm convergence. If the error reduction rate is smaller than\n",
        "        min_gain for val_step iterations, the algorithm converges.\n",
        "    normDir: a string indicating the directory to save result files.\n",
        "    error: a string indicating the function to evaluate the difference between feature distance ranking and pixel\n",
        "        distance ranking. 'abs' indicates the absolute function. 'squared' indicates the square function.\n",
        "    switch_t: the threshold on error change rate. Error change rate is\n",
        "        (error before feature swapping - error after feature swapping) / error before feature swapping.\n",
        "        In each iteration, if the largest error change rate resulted from all possible feature swappings\n",
        "        is not smaller than switch_t, the feature swapping resulting in the largest error change rate will\n",
        "        be performed. If switch_t >= 0, the IGTD algorithm monotonically reduces the error during optimization.\n",
        "    min_gain: if the error reduction rate is not larger than min_gain for val_step iterations, the algorithm converges.\n",
        "\n",
        "    Return:\n",
        "    This function does not return any variable, but saves multiple result files, which are the following\n",
        "    1.  Results.pkl stores the original tabular data, the generated image data, and the names of samples. The generated\n",
        "        image data is a 3D numpy array. Its size is [number of pixel rows in image, number of pixel columns in image,\n",
        "        number of samples]. The range of values is [0, 255]. Small values in the array actually correspond to high\n",
        "        values in the tabular data.\n",
        "    2.  Results_Auxiliary.pkl stores the ranking matrix of pairwise feature distances before optimization,\n",
        "        the ranking matrix of pairwise pixel distances, the coordinates of pixels when concatenating pixels\n",
        "        row by row from image to form the pixel distance ranking matrix, error in each iteration,\n",
        "        and time (in seconds) when completing each iteration.\n",
        "    3.  original_feature_ranking.png shows the feature distance ranking matrix before optimization.\n",
        "    4.  image_ranking.png shows the pixel distance ranking matrix.\n",
        "    5.  error_and_runtime.png shows the change of error vs. time during the optimization process.\n",
        "    6.  error_and_iteration.png shows the change of error vs. iteration during the optimization process.\n",
        "    7.  optimized_feature_ranking.png shows the feature distance ranking matrix after optimization.\n",
        "    8.  data folder includes two image data files for each sample. The txt file is the image data in matrix format,\n",
        "        in which high values correspond to high values of features in tabular data. The png file shows the\n",
        "        visualization of image data, in which black and white correspond to high and low values of features in\n",
        "        tabular data, respectively.\n",
        "    '''\n",
        "\n",
        "    if os.path.exists(normDir):\n",
        "        shutil.rmtree(normDir)\n",
        "    os.mkdir(normDir)\n",
        "\n",
        "    ranking_feature, corr = generate_feature_distance_ranking(data=norm_d, method=fea_dist_method)\n",
        "    fig = plt.figure(figsize=(save_image_size, save_image_size))\n",
        "    plt.imshow(np.max(ranking_feature) - ranking_feature, cmap='gray', interpolation='nearest')\n",
        "    plt.savefig(fname=normDir + '/original_feature_ranking.png', bbox_inches='tight', pad_inches=0)\n",
        "    plt.close(fig)\n",
        "\n",
        "    coordinate, ranking_image = generate_matrix_distance_ranking(num_r=scale[0], num_c=scale[1],\n",
        "                                                                 method=image_dist_method, num=norm_d.shape[1])\n",
        "    fig = plt.figure(figsize=(save_image_size, save_image_size))\n",
        "    plt.imshow(np.max(ranking_image) - ranking_image, cmap='gray', interpolation='nearest')\n",
        "    plt.savefig(fname=normDir + '/image_ranking.png', bbox_inches='tight', pad_inches=0)\n",
        "    plt.close(fig)\n",
        "\n",
        "    index, err, time = IGTD(source=ranking_feature, target=ranking_image,\n",
        "        err_measure=error, max_step=max_step, switch_t=switch_t, val_step=val_step, min_gain=min_gain, random_state=1,\n",
        "        save_folder=normDir + '/' + error, file_name='')\n",
        "\n",
        "    fig = plt.figure()\n",
        "    plt.plot(time, err)\n",
        "    plt.savefig(fname=normDir + '/error_and_runtime.png', bbox_inches='tight', pad_inches=0)\n",
        "    plt.close(fig)\n",
        "    fig = plt.figure()\n",
        "    plt.plot(range(len(err)), err)\n",
        "    plt.savefig(fname=normDir + '/error_and_iteration.png', bbox_inches='tight', pad_inches=0)\n",
        "    plt.close(fig)\n",
        "    min_id = np.argmin(err)\n",
        "    ranking_feature_random = ranking_feature[index[min_id, :], :]\n",
        "    ranking_feature_random = ranking_feature_random[:, index[min_id, :]]\n",
        "\n",
        "    fig = plt.figure(figsize=(save_image_size, save_image_size))\n",
        "    plt.imshow(np.max(ranking_feature_random) - ranking_feature_random, cmap='gray',\n",
        "               interpolation='nearest')\n",
        "    plt.savefig(fname=normDir + '/optimized_feature_ranking.png', bbox_inches='tight', pad_inches=0)\n",
        "    plt.close(fig)\n",
        "\n",
        "    data, samples = generate_image_data(data=norm_d, index=index[min_id, :], num_row=scale[0], num_column=scale[1],\n",
        "        coord=coordinate, image_folder=normDir + '/data', file_name='')\n",
        "\n",
        "    output = open(normDir + '/Results.pkl', 'wb')\n",
        "    cp.dump(norm_d, output)\n",
        "    cp.dump(data, output)\n",
        "    cp.dump(samples, output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(normDir + '/Results_Auxiliary.pkl', 'wb')\n",
        "    cp.dump(ranking_feature, output)\n",
        "    cp.dump(ranking_image, output)\n",
        "    cp.dump(coordinate, output)\n",
        "    cp.dump(err, output)\n",
        "    cp.dump(time, output)\n",
        "    output.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4ac4a72",
      "metadata": {
        "scrolled": true,
        "id": "f4ac4a72",
        "outputId": "35e18f9f-7a00-414d-f9c8-3d4be22f2e37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\91994\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "C:\\Users\\91994\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0 err: 245304118830209.5\n",
            "Step 1 err: 245304118830209.5\n",
            "Step 2 err: 245304118830209.5\n",
            "Step 3 err: 245304118830209.5\n",
            "Step 4 err: 245304118830209.5\n",
            "Step 5 err: 245304118830209.5\n",
            "Step 6 err: 245304118830209.5\n",
            "Step 7 err: 245304118830209.5\n",
            "Step 8 err: 245304118830209.5\n",
            "Step 9 err: 245304118830209.5\n",
            "Step 10 err: 245304118830209.5\n",
            "Step 11 err: 245304118830209.5\n",
            "Step 12 err: 245304118830209.5\n",
            "Step 13 err: 245304118830209.5\n",
            "Step 14 err: 245304118830209.5\n",
            "Step 15 err: 245304118830209.5\n",
            "Step 16 err: 245304118830209.5\n",
            "Step 17 err: 245304118830209.5\n",
            "Step 18 err: 245304118830209.5\n",
            "Step 19 err: 245304118830209.5\n",
            "Step 20 err: 245304118830209.5\n",
            "Step 21 err: 245304118830209.5\n",
            "Step 22 err: 245304118830209.5\n",
            "Step 23 err: 245304118830209.5\n",
            "Step 24 err: 245304118830209.5\n",
            "Step 25 err: 245304118830209.5\n",
            "Step 26 err: 245304118830209.5\n",
            "Step 27 err: 245304118830209.5\n",
            "Step 28 err: 245304118830209.5\n",
            "Step 29 err: 245304118830209.5\n",
            "Step 30 err: 245304118830209.5\n",
            "Step 31 err: 245304118830209.5\n",
            "Step 32 err: 245304118830209.5\n",
            "Step 33 err: 245304118830209.5\n",
            "Step 34 err: 245304118830209.5\n",
            "Step 35 err: 245304118830209.5\n",
            "Step 36 err: 245304118830209.5\n",
            "Step 37 err: 245304118830209.5\n",
            "Step 38 err: 245304118830209.5\n",
            "Step 39 err: 245304118830209.5\n",
            "Step 40 err: 245304118830209.5\n",
            "Step 41 err: 245304118830209.5\n",
            "Step 42 err: 245304118830209.5\n",
            "Step 43 err: 245304118830209.5\n",
            "Step 44 err: 245304118830209.5\n",
            "Step 45 err: 245304118830209.5\n",
            "Step 46 err: 245304118830209.5\n",
            "Step 47 err: 245304118830209.5\n",
            "Step 48 err: 245304118830209.5\n",
            "Step 49 err: 245304118830209.5\n",
            "Step 50 err: 245304118830209.5\n",
            "Step 51 err: 245304118830209.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\91994\\AppData\\Local\\Temp\\ipykernel_16516\\353911491.py:154: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  index_record = index_record[:len(err_record), :].astype(np.int)\n"
          ]
        }
      ],
      "source": [
        "table_to_image(norm_d=XAA, scale = [24,24], fea_dist_method = 'Pearson', image_dist_method = 'Eucledian', save_image_size=5, max_step=1000, val_step = 50, normDir='pictureAA',\n",
        "                   error='squared', switch_t=0, min_gain=0.00001)\n",
        "np.save(\"residualAA\",yAA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e9cd7c6",
      "metadata": {
        "id": "0e9cd7c6",
        "outputId": "ff18b083-dd14-4f28-8967-32c730281a1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\91994\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "C:\\Users\\91994\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0 err: 237137921289416.0\n",
            "Step 1 err: 237137921289416.0\n",
            "Step 2 err: 237137921289416.0\n",
            "Step 3 err: 237137921289416.0\n",
            "Step 4 err: 237137921289416.0\n",
            "Step 5 err: 237137921289416.0\n",
            "Step 6 err: 237137921289416.0\n",
            "Step 7 err: 237137921289416.0\n",
            "Step 8 err: 237137921289416.0\n",
            "Step 9 err: 237137921289416.0\n",
            "Step 10 err: 237137921289416.0\n",
            "Step 11 err: 237137921289416.0\n",
            "Step 12 err: 237137921289416.0\n",
            "Step 13 err: 237137921289416.0\n",
            "Step 14 err: 237137921289416.0\n",
            "Step 15 err: 237137921289416.0\n",
            "Step 16 err: 237137921289416.0\n",
            "Step 17 err: 237137921289416.0\n",
            "Step 18 err: 237137921289416.0\n",
            "Step 19 err: 237137921289416.0\n",
            "Step 20 err: 237137921289416.0\n",
            "Step 21 err: 237137921289416.0\n",
            "Step 22 err: 237137921289416.0\n",
            "Step 23 err: 237137921289416.0\n",
            "Step 24 err: 237137921289416.0\n",
            "Step 25 err: 237137921289416.0\n",
            "Step 26 err: 237137921289416.0\n",
            "Step 27 err: 237137921289416.0\n",
            "Step 28 err: 237137921289416.0\n",
            "Step 29 err: 237137921289416.0\n",
            "Step 30 err: 237137921289416.0\n",
            "Step 31 err: 237137921289416.0\n",
            "Step 32 err: 237137921289416.0\n",
            "Step 33 err: 237137921289416.0\n",
            "Step 34 err: 237137921289416.0\n",
            "Step 35 err: 237137921289416.0\n",
            "Step 36 err: 237137921289416.0\n",
            "Step 37 err: 237137921289416.0\n",
            "Step 38 err: 237137921289416.0\n",
            "Step 39 err: 237137921289416.0\n",
            "Step 40 err: 237137921289416.0\n",
            "Step 41 err: 237137921289416.0\n",
            "Step 42 err: 237137921289416.0\n",
            "Step 43 err: 237137921289416.0\n",
            "Step 44 err: 237137921289416.0\n",
            "Step 45 err: 237137921289416.0\n",
            "Step 46 err: 237137921289416.0\n",
            "Step 47 err: 237137921289416.0\n",
            "Step 48 err: 237137921289416.0\n",
            "Step 49 err: 237137921289416.0\n",
            "Step 50 err: 237137921289416.0\n",
            "Step 51 err: 237137921289416.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\91994\\AppData\\Local\\Temp\\ipykernel_16516\\353911491.py:154: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  index_record = index_record[:len(err_record), :].astype(np.int)\n"
          ]
        }
      ],
      "source": [
        "table_to_image(norm_d=XEUR, scale = [24,24], fea_dist_method = 'Pearson', image_dist_method = 'Eucledian', save_image_size=5, max_step=1000, val_step = 50, normDir='pictureEUR',\n",
        "                   error='squared', switch_t=0, min_gain=0.00001)\n",
        "np.save(\"residualEUR\",yEUR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e022db",
      "metadata": {
        "id": "35e022db"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}